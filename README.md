# Hand Gesture Recognition using CNN  

This project implements a **Convolutional Neural Network (CNN)** for recognizing **American Sign Language (ASL)** hand gestures. The system classifies grayscale images of hand signs (Aâ€“Z, excluding J and Z due to motion) into their respective letters, aiming to support communication accessibility for the hearing-impaired.  

---

## ğŸ“Š Dataset  
- **Images:** 28Ã—28 grayscale hand gesture images.  
- **Classes:** 26 letters (Aâ€“Z except J and Z).  
- **Format:** Similar to MNIST, with labeled training and testing sets.  

---

## ğŸ—ï¸ Model Architecture  
The CNN consists of:  
1. **Conv Layer 1 & 2** â€“ Extracts low- and high-level image features.  
2. **ReLU Activation** â€“ Introduces non-linearity.  
3. **MaxPooling (2Ã—2)** â€“ Reduces spatial dimensions.  
4. **Fully Connected Layer** â€“ Learns feature combinations.  
5. **Dropout (0.28)** â€“ Reduces overfitting.  
6. **Output Layer (26 nodes, Softmax)** â€“ Classifies into letters.  

- **Optimizer:** Adam  
- **Training:** 20 epochs  

---

## ğŸ“ˆ Results  
- **Training Accuracy:** ~98%  
- **Testing Accuracy:** ~96%  
- **Loss & Accuracy Curves:** Show steady convergence.  
- **Overfitting Control:** Dropout improved generalization; additional dropout (0.4) further reduced variance.  

---

## ğŸ” Observations  
- CNN captured hierarchical features effectively (from edges to complex patterns).  
- Pooling layers provided **translation invariance**, improving robustness.  
- Weight sharing reduced computational cost while maintaining accuracy.  

---

## âš¡ How to Run  
1. Clone repository:  
   ```bash
   git clone https://github.com/your-username/hand-gesture-recognition-cnn.git
   cd hand-gesture-recognition-cnn
